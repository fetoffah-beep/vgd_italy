{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to spyder (Python 3.12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9651c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e796e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = gpd.read_file(r\"C:\\Users\\gmfet\\Desktop\\predictors\\lithology\\litology_italy.gpkg\")\n",
    "output_path = r'C:\\Users\\gmfet\\Desktop\\collaboration_with_Awais\\lithology.tif'\n",
    "\n",
    "# Get the bounds of the file\n",
    "xmin, ymin, xmax, ymax = lithology.total_bounds\n",
    "res = 0.001  # pixel size\n",
    "\n",
    "# width = int((xmax - xmin) / res)\n",
    "height = int((ymax - ymin) / res)\n",
    "width = height\n",
    "transform = from_bounds(xmin, ymin, xmax, ymax, width, height)\n",
    "\n",
    "shapes = ((geom, value) for geom, value in zip(lithology.geometry, lithology[\"cat\"]))\n",
    "\n",
    "with rasterio.open(output_path, 'w', driver='GTiff', width=width, height=height, count=1, crs=lithology.crs, transform=transform, dtype='uint8') as geo_pckg:\n",
    "    geo_pckg.write(rasterize(shapes, out_shape=(height, width), fill=0, transform=transform, dtype='uint8'), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d98896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# italy shapefile \n",
    "shp_path = r\"C:\\Users\\gmfet\\vgd_italy\\italy_aoi\\gadm41_ITA_0.shp\"\n",
    "\n",
    "shp_file = gpd.read_file(shp_path)\n",
    "\n",
    "shp_file.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd17025",
   "metadata": {},
   "source": [
    "## Static features\n",
    "Save the categorical variables as uint8 dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513eb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "filepath = r'C:\\Users\\gmfet\\vgd_italy\\regression\\original_data'\n",
    "\n",
    "file_list = os.listdir(filepath)\n",
    "for file in file_list:\n",
    "    if file.endswith('.tif'):\n",
    "        engine='rasterio'\n",
    "    elif file.endswith('.nc'):\n",
    "        engine = 'h5netcdf'\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    with xr.open_dataset(os.path.join(filepath, file), engine=engine) as data_ds:\n",
    "        rename_dict = {'x': 'longitude', 'y': 'latitude', 'lon': 'longitude', 'lat': 'latitude', 'band':'time'}\n",
    "        data_ds = data_ds.rename({k: v for k, v in rename_dict.items() if k in data_ds.dims})\n",
    "        print(data_ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4b2c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 104MB\n",
      "Dimensions:  (lon: 7200, lat: 3600)\n",
      "Coordinates:\n",
      "  * lon      (lon) float64 58kB -180.0 -179.9 -179.9 ... 179.9 179.9 180.0\n",
      "  * lat      (lat) float64 29kB 89.97 89.92 89.88 89.82 ... -89.88 -89.92 -89.97\n",
      "Data variables:\n",
      "    crs      int32 4B ...\n",
      "    Band1    (lat, lon) float32 104MB dask.array<chunksize=(3600, 7200), meta=np.ndarray>\n",
      "Attributes:\n",
      "    CDI:                        Climate Data Interface version 1.9.8 (https:/...\n",
      "    Conventions:                CF-1.5\n",
      "    history:                    Wed Dec 16 16:42:50 2020: cdo -mulc,10. ksat3...\n",
      "    GDAL_AREA_OR_POINT:         Area\n",
      "    GDAL:                       GDAL 3.0.4, released 2020/01/28\n",
      "    history_of_appended_files:  Fri Dec 11 16:53:00 2020: Appended file /huge...\n",
      "    NCO:                        netCDF Operators version 4.9.2 (Homepage = ht...\n",
      "    CDO:                        Climate Data Operators version 1.9.8 (https:/...\n"
     ]
    }
   ],
   "source": [
    "# with rxr.open_rasterio(r\"C:\\Users\\gmfet\\Desktop\\predictors\\LULC\\*.tif\", chunks=1000) as ds:\n",
    "#     ds\n",
    "with xr.open_mfdataset(r\"C:\\Users\\gmfet\\vgd_italy\\regression\\original_data\\ksat.nc\", engine='netcdf4') as ds:\n",
    "    print(ds)\n",
    "    ds = ds.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "\n",
    "    # ds['band_data'] = ds['band_data'].rio.write_nodata(np.nan)\n",
    "    # ds = ds.astype('float32')\n",
    "    # ds = ds.rio.clip(shp_file.geometry, shp_file.crs, drop=True, all_touched=True)\n",
    "    \n",
    "    data_attrs = ds.attrs.copy()\n",
    "    coord_attrs = {c: ds[c].attrs.copy() for c in ds.coords}\n",
    "    # ds = ds.squeeze(\"band\", drop=True)\n",
    "    # # # ds = ds.drop_vars(\"spatial_ref\")\n",
    "    ds = ds.rename({\"lon\": \"longitude\", \"lat\": \"latitude\", 'Band1': 'ksat'})\n",
    "    ds = ds.chunk({'latitude': 256, 'longitude': 256})\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # ds = ds.rename({\"x\": \"longitude\", \"y\": \"latitude\", 'band': 'time', 'band_data': 'drought_code'})\n",
    "    # ds['time'] = pd.date_range('2017-01-01', periods=ds.sizes['time'], freq='D')\n",
    "    # ds = ds.chunk({'time': -1})\n",
    "    # ds = ds.interpolate_na(\n",
    "    #     dim=\"time\", \n",
    "    #     method=\"polynomial\", \n",
    "    #     order=3,\n",
    "    #     fill_value=\"extrapolate\"\n",
    "        \n",
    "    # )\n",
    "\n",
    "    # ds = ds.chunk({'time': -1, 'latitude': 256, 'longitude': 256})\n",
    "\n",
    "    # # # # # # ds = ds.to_dataset(name='clay_content')\n",
    "    ds.attrs.update(data_attrs)\n",
    "    ds = ds.astype(\"float32\")\n",
    "\n",
    "    # restore coordinate attributes\n",
    "    for c in [\"longitude\", \"latitude\"]:\n",
    "        if c in coord_attrs:\n",
    "            ds[c].attrs.update(coord_attrs[c])\n",
    "\n",
    "    # # # ds = ds.dropna('latitude')\n",
    "\n",
    "    # # # ds = ds.rio.clip(shp_file.geometry, shp_file.crs, drop=True, all_touched=True)\n",
    "            \n",
    "    ds.to_netcdf(\"ksat.nc\", engine=\"h5netcdf\", encoding= {'ksat': {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 0, \"compression\": \"gzip\", \"compression_opts\": 0}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a3152",
   "metadata": {},
   "source": [
    "## Dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rxr.open_rasterio(r\"C:\\Users\\gmfet\\Desktop\\predictors\\LULC\\*.tif\", chunks=1000) as ds:\n",
    "#     ds\n",
    "# from dask.distributed import Client\n",
    "# client = Client() # This will give you a link to a dashboard (usually http://localhost:8787)\n",
    "\n",
    "encoding = {\n",
    "    \"temperature\": {\n",
    "        \"dtype\": \"float32\",        # Store as 16-bit integers\n",
    "        \"scale_factor\": 0.01,   # Preservation of 2 decimal places\n",
    "        # \"_FillValue\": -9999,    # Map NaNs to -9999\n",
    "        \"zlib\": True,           # Enable compression\n",
    "        \"complevel\": 0,         # Moderate compression level\n",
    "        # \"shuffle\": True,        # Better compression efficiency\n",
    "        # \"chunksizes\": (1, 180, 360) # Optimize for time-series access\n",
    "    }\n",
    "}\n",
    "\n",
    "# ds.to_netcdf(\"output.nc\", encoding=encoding)\n",
    "with xr.open_mfdataset(r\"C:\\Users\\gmfet\\vgd_italy\\data\\dynamic\\temperature.tif\", engine='rasterio') as ds:\n",
    "    ds\n",
    "    ds['band_data'] = ds['band_data'].rio.write_nodata(np.nan)\n",
    "    # ds = ds.chunk({'x': 1024, 'y': 1024})\n",
    "    ds = ds.astype('float32')\n",
    "    # # # # # ds = ds.rio.clip(shp_file.geometry, shp_file.crs, drop=True, all_touched=True)\n",
    "    \n",
    "    # ds = ds.fillna(0).astype(\"float32\")\n",
    "    data_attrs = ds.attrs.copy()\n",
    "    coord_attrs = {c: ds[c].attrs.copy() for c in ds.coords}\n",
    "    # # # ds = ds.drop_vars(\"spatial_ref\")\n",
    "    ds = ds.rename({\"x\": \"longitude\", \"y\": \"latitude\", 'band': 'time', 'band_data': 'drought_code'})\n",
    "    ds['time'] = pd.date_range('2017-01-01', periods=ds.sizes['time'], freq='D')\n",
    "    # ds = ds.chunk({'longitude': -1, 'latitude': 1024, 'time': 512})\n",
    "\n",
    "    # ds = ds.interpolate_na(\n",
    "    #     dim=\"longitude\", \n",
    "    #     method=\"linear\", \n",
    "    #     fill_value=\"extrapolate\"\n",
    "        \n",
    "    # )\n",
    "\n",
    "    ds = ds.chunk({'time': -1, 'latitude': 1024, 'longitude': 1024,})\n",
    "\n",
    "    # ds = ds.interpolate_na(\n",
    "    #     dim=\"time\", \n",
    "    #     method=\"linear\", \n",
    "    #     fill_value=\"extrapolate\"\n",
    "        \n",
    "    # )\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # # # # # # # ds = ds.to_dataset(name='bulk_density')\n",
    "    ds.attrs.update(data_attrs)\n",
    "    ds = ds.astype(\"float32\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # restore coordinate attributes\n",
    "    for c in [\"longitude\", \"latitude\"]:\n",
    "        if c in coord_attrs:\n",
    "            ds[c].attrs.update(coord_attrs[c])\n",
    "\n",
    "    # # # # ds = ds.dropna('latitude')\n",
    "\n",
    "    # # # # ds = ds.rio.clip(shp_file.geometry, shp_file.crs, drop=True, all_touched=True)\n",
    "    \n",
    "    ds.to_netcdf(\"temperature.nc\", engine=\"h5netcdf\", encoding= encoding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42575f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.astype(\"uint8\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d95e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['temperature'].sel(time='2018-08-01').plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63609e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.unique(ds['drought_code'].sel(time='2017-01-01').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e7d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
