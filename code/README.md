# Project Overview

This project is designed for **Vertical Ground Displacement** studies in Italy, from data preprocessing and model training to evaluation and interpretation. It contains the necessary components to handle data, build and train machine learning models, save/load model checkpoints, evaluate performance, and interpret results.

---

## Project Structure

The project is structured as follows:

### 1. **Data Directory (`data/`)**
- **Purpose**: Store and manage raw, processed, and external data.
- **Subdirectories**:
  - **`raw/`**: Raw data files (e.g., CSVs, images).
  - **`processed/`**: Cleaned and processed data ready for model training.
  - **`external/`**: External datasets not directly generated by this project.
  - **`interim/`**: Temporary files for intermediate data processing steps.

### 2. **Notebooks Directory (`notebooks/`)**
- **Purpose**: Store Jupyter Notebooks for experimentation, analysis, and visualization.
- **Files**:
  - **`eda.ipynb`**: Exploratory Data Analysis (EDA) notebook for summarizing and visualizing data.
  - **`experiments.ipynb`**: Notebook for testing different machine learning models, including hyperparameter tuning and comparisons.

### 3. **Source Code Directory (`src/`)**
Contains the main scripts for preprocessing data, training models, and performing evaluations and interpretations.
- **`src/data/`**:
  - **`load_data.py`**: Functions for loading raw data from various sources (CSV, JSON, etc.).
  - **`preprocess_data.py`**: Functions for cleaning, transforming, and encoding the data (e.g., handling missing values).
  - **`dataloader.py`**: Defines custom datasets and dataloaders for PyTorch, including the logic for batching and shuffling the data.
  
- **`src/features/`**:
  - **`feature_engineering.py`**: Includes functions to create new features or transform existing ones for model input.
  - **`feature_selection.py`**: Functions for selecting important features using methods like SelectKBest or PCA.
  
- **`src/models/`**:
  - **`train_model.py`**: Main script for training machine learning models. It includes code for defining the model, training loop, and saving checkpoints.
  - **`checkpoint.py`**: Code to save and load model checkpoints during training. Helps in resuming training or evaluating models.
  
- **`src/utils/`**:
  - **`utils.py`**: General utility functions (e.g., reading configuration files).
  - **`logger.py`**: Utility for logging messages during model training or data processing.
  
- **`src/evaluation/`**:
  - **`metrics.py`**: Functions for evaluating model performance using various metrics like RMSE, accuracy, etc.
  - **`visualize_results.py`**: Functions for visualizing model predictions, feature importance, etc.
  
- **`src/interpretation/`**:
  - **`shap_analysis.py`**: Implementation of SHAP (Shapley Additive Explanations) for model interpretability.
  - **`lime_analysis.py`**: LIME (Local Interpretable Model-Agnostic Explanations) for interpreting model predictions.

### 4. **Models Directory (`models/`)**
- **Purpose**: Store serialized models and associated metadata.
  - **`model_v1.pkl`**: Placeholder for serialized machine learning models.
  - **`model_metadata.json`**: JSON file containing metadata for the trained model (e.g., hyperparameters, version).
  
### 5. **Logs Directory (`models/logs/`)**
- **Purpose**: Store logs related to model training, including training history.
  - **`training_log.txt`**: Log file containing the output of training epochs, loss, and other key metrics.

### 6. **Outputs Directory (`outputs/`)**
- **Purpose**: Store output results such as predictions and charts.
  - **`predictions.csv`**: Final model predictions on test data.
  - **`charts/`**: Directory for saving model performance plots, feature importance charts, etc.

### 7. **Tests Directory (`tests/`)**
- **Purpose**: Contains unit tests for verifying data processing and model training scripts.
  - **`test_data.py`**: Unit tests for functions in `load_data.py` and `preprocess_data.py`.
  - **`test_models.py`**: Unit tests for functions in `train_model.py`.

### 8. **Configuration Files (`config/`)**
- **Purpose**: Store configuration files for the project.
  - **`config.yaml`**: YAML file containing settings for data paths, model configurations, etc.
  - **`hyperparameters.json`**: JSON file for defining hyperparameters for model training.

### 9. **Documentation Directory (`docs/`)**
- **Purpose**: Store project documentation and guides.
  - **`setup_guide.md`**: Step-by-step guide for setting up the project environment.
  - **`methodology.md`**: Detailed explanation of the projectâ€™s methodology, including models and algorithms used.

---

## Steps to Follow

### 1. **Setup the Environment**
1. Clone the repository:
    ```bash
    git clone https://github.com/your-repo/ml-project.git
    cd ml-project
    ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Set up any environment-specific configurations (e.g., paths to data) in the `config/config.yaml` file.

### 2. **Data Preparation**
1. Place the raw data in the `data/raw/` directory.
2. Use the `load_data.py` script to load and validate the data.
3. Preprocess the data with `preprocess_data.py` to clean and transform it, saving the processed data to `data/processed/`.

### 3. **Training the Model**
1. Define and initialize your model in `train_model.py`.
2. Train the model by running the script:
    ```bash
    python src/models/train_model.py
    ```
   This will:
   - Load the data using `dataloader.py`.
   - Train the model and save checkpoints using `checkpoint.py`.
   - Save trained models and logs.

3. If you want to resume training from a checkpoint, specify the path to the checkpoint file.

### 4. **Model Evaluation**
1. After training, evaluate the model using `metrics.py`.
2. Visualize the model's performance and feature importance with `visualize_results.py`.

### 5. **Interpretation**
1. Use `shap_analysis.py` for SHAP-based model interpretability or `lime_analysis.py` for LIME-based explanation.

### 6. **Testing**
1. Run unit tests to verify the functionality of the data and model processing scripts:
    ```bash
    pytest tests/
    ```

---

## Additional Information

For more details about each component of the project, refer to the corresponding file and its description above.

---

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.
